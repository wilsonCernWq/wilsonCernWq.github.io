{
    "collection": "publications",
    "venues": {
        "VIS": "IEEE Visualization Conference (VIS)",
        "VIS_tutorial": "IEEE Visualization Conference (VIS) Tutorial",
        "EGPGV": "The Eurographics Symposium on Parallel Graphics and Visualization (EGPGV)",
        "LDAV": "IEEE Symposium on Large Data Analysis and Visualization (LDAV)",
        "LDAV_poster": "IEEE Symposium on Large Data Analysis and Visualization (LDAV) Poster",
        "EuroVis": "The Eurographics Conference on Visualization (EuroVis)",
        "arxiv": "ArXiv Preprint",
        "TVCG": "IEEE Transactions on Visualization and Computer Graphics (TVCG)"
    },
    "data": [
        {
            "title": "HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural Representations via Knowledge Distillation",
            "venue": "Submitted for Publication",
            "authors": "Qi Wu, David Bauer, Yuyang Chen, and Kwan Liu Ma",
            "abstract": "Implicit Neural Representations (INRs) have recently exhibited immense potential in the field of scientific visualization for both data generation and visualization tasks. However, these representations often consist of large multi-layer perceptrons (MLPs), necessitating millions of operations for a single forward pass, consequently hindering interactive visual exploration. While reducing the size of the MLPs and employing efficient parametric encoding schemes can alleviate this issue, it compromises generalizability for unseen parameters, rendering it unsuitable for tasks such as temporal super-resolution. In this paper, we introduce HyperINR, a novel hypernetwork architecture capable of directly predicting the weights for a compact INR. By harnessing an ensemble of multiresolution hash encoding units in unison, the resulting INR attains state-of-the-art inference performance (up to 100x higher inference bandwidth) and can support interactive photo-realistic volume visualization. Additionally, by incorporating knowledge distillation, exceptional data and visualization generation quality is achieved, making our method valuable for real-time parameter exploration. We validate the effectiveness of the HyperINR architecture through a comprehensive ablation study. We showcase the versatility of HyperINR across three distinct scientific domains: novel view synthesis, temporal super-resolution of volume data, and volume rendering with dynamic global shadows. By simultaneously achieving efficiency and generalizability, HyperINR paves the way for applying INR in a wider array of scientific visualization applications.",
            "preprint_url": "https://drive.google.com/file/d/1CY2WSxNTNXOap0vDW4euh4S6xJQf8UE7/view?usp=share_link",
            "arxiv_url": "https://arxiv.org/abs/2304.04188",
            "preview": "arxiv-hyperinr.png",
            "teaser": "arxiv-hyperinr-teaser.png",
            "teaser_caption": "Figure 1: Comparisons between HyperINR and data interpolation for the temporal super-resolution task using the vortices dataset. Listed timesteps are midpoints of different interpolation intervals. HyperINR can directly predict the weights of a regular implicit neural representation (INR) for unseen parameters. The predicted INR is in general more accurate than data interpolation results and can support interactive volumetric path tracing.",
            "permalink": "arxiv-hyperinr",
            "date": "2023-03-31"
        },
        {
            "title": "Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination",
            "venue": "Submitted for Publication",
            "authors": "David Bauer, Qi Wu, and Kwan Liu Ma",
            "abstract": "Volume data is fundamental in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks---a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method's performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to state-of-the-art scientific rendering techniques. Our findings show that Photon Field Networks can faithfully represent indirect global illumination across the phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.",
            "preprint_url": "https://drive.google.com/file/d/1R6bVLlHLQfcy-evgUDIdIMKCJkZzuC45/view?usp=share_link",
            "preview": "arxiv-photon-field.png",
            "teaser": "arxiv-photon-field-teaser.png",
            "teaser_caption": "Figure 1: Our photon field networks replace the global illumination term of the rendering equation. With our approach we are able to achieve comparable results for volumetric rendering in a fraction of the time it takes a conventional path tracer. The photon fields are light-weight and can be trained in seconds.",
            "permalink": "arxiv-photon-field",
            "date": "2023-03-30"
        },
        {
            "title": "Memory-Efficient GPU Volume Path Tracing of AMR Data Using the Dual Mesh",
            "venue": "@EuroVis",
            "authors": "Stefan Zellmann, Qi Wu, Kwan-Liu Ma, and Ingo Wald",
            "abstract": "A common way to render cell-centric adaptive mesh refinement (AMR) data is to compute the dual mesh and visualize that with a standard unstructured element renderer. While the dual mesh provides a high-quality interpolator, the memory requirements of the dual mesh data structure are significantly higher than those of the original grid, which prevents rendering very large data sets. We introduce a GPU-friendly data structure and a clustering algorithm that allow for efficient AMR dual mesh rendering with a competitive memory footprint. Fundamentally, any off-the-shelf unstructured element renderer running on GPUs could be extended to support our data structure just by adding a gridlet element type in addition to the standard tetrahedra, pyramids, wedges, and hexahedra supported by default. We integrated the data structure into a volumetric path tracer to compare it to various state-of-the-art unstructured element sampling methods. We show that our data structure easily competes with these method in terms of rendering performance, but is much more memory-efficient.",
            "preprint_url": "https://drive.google.com/file/d/1oW1ZMzcH_Yxh7ST7gMfreQqTFrhlpmWo/view?usp=share_link",
            "preview": "eurovis2023-stitcher.png",
            "teaser": "eurovis2023-stitcher-teaser.png",
            "teaser_caption": "Figure 1: Overview of our method. Given a block-structured or octree-AMR data set (left) we first create the dual mesh (middle) and split that into the truly unstructured elements used to stitch the level boundaries (red) and those that are regular voxels (blue/white checkered). We then cluster voxels to become agridletso. Right: gridlets colorized by their ID. We build a bounding volume hierarchy over the gridlets and the remaining unstructured elements. The result is a sampleable representation that generates the exact same result as sampling on the dual mesh directly, but with significantly lower memory overhead and higher sampling speed. On our largest data sets, we see memory savings of up to 3x compared to highly compressed state-of-the-art unstructured mesh representations.",
            "permalink": "eurovis2023-stitcher",
            "date": "2023-02-27"
        },
        {
            "title": "Distributed Neural Representation for Reactive in situ Visualization",
            "venue": "Submitted for Publication",
            "authors": "Qi Wu, Joseph A. Insley, Victor A. Mateevitsi, Silvio Rizzi, Michael E. Papka, and Kwan Liu Ma",
            "abstract": "In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.",
            "preprint_url": "https://drive.google.com/file/d/1F2vJHvjoqXfhkIe4Glj6EcSNSQNx1pd7/view?usp=share_link",
            "preview": "egpgv2023-dvnr-reactive.png",
            "teaser": "egpgv2023-dvnr-reactive-teaser.png",
            "teaser_caption": "Figure 1: We compared the rendering of our distributed neural representations using varying numbers of training steps. The data was distributed to two MPI ranks and trained using two NVIDIA A100-40G GPUs on the ALCF Polaris supercomputer. Partition boundaries were highlighted using white lines in A) and B). C) are zoomed views of A) near partition boundaries. In 1C), an obvious discontinuity is visible at the partition boundary. With more training steps in 2), the discontinuity becomes less obvious, but high frequency noises are still visible. However, in 3), with sufficient training steps, these artifacts are no longer visible. We used flow field data generated from the S3D simulation for this experiment.",
            "permalink": "egpgv2023-dvnr-reactive",
            "date": "2023-03-11"
        },
        {
            "title": "Beyond ExaBricks: GPU Volume Path Tracing of AMR Data",
            "venue": "@arxiv",
            "authors": "Stefan Zellmann, Qi Wu, Alper Sahistan, Kwan-Liu Ma, and Ingo Wald",
            "abstract": "Adaptive Mesh Refinement (AMR) is becoming a prevalent data representation for scientific visualization. Resulting from large fluid mechanics simulations, the data is usually cell centric, imposing a number of challenges for high quality reconstruction at sample positions. While recent work has concentrated on real-time volume and isosurface rendering on GPUs, the rendering methods used still focus on simple lighting models without scattering events and global illumination. As in other areas of rendering, key to real-time performance are acceleration data structures; in this work we analyze the major bottlenecks of data structures that were originally optimized for camera/primary ray traversal when used with the incoherent ray tracing workload of a volumetric path tracer, and propose strategies to overcome the challenges coming with this.",
            "preprint_url": "https://drive.google.com/file/d/1-nYUIsiEqGLvnYhrMomyycOQyF-T5OFu/view?usp=share_link",
            "arxiv_url": "https://arxiv.org/abs/2211.09997",
            "preview": "arxiv-beyond.png",
            "teaser": "arxiv-beyond-teaser.png",
            "teaser_caption": "Figure 1: AMR rendering comparison using ExaBrick vs. our extended framework. Top-left: original 'sci-vis style' renderingwith ray marching, local shading with on-the-fly gradients, and a delta light source. Bottom-right: volumetric path tracing with multiscattering, isotropic phase function and ambient lighting. The original software used two RTX 8000 GPUs to render a convergence frame with allquality settings set to maximum at 4 frames/sec.; our framework, with the best combination of optimizations discussed in this paper, renderspath-traced convergence frames with full global illumination at 6.7 frames/sec.",
            "bibtex": "@article{zellmann2022beyond,\\n    title={Beyond ExaBricks: GPU Volume Path Tracing of AMR Data},\\n    author={Zellmann, Stefan and Wu, Qi and Sahistan, Alper and Ma, Kwan-Liu and Wald, Ingo},\\n    journal={arXiv preprint arXiv:2211.09997},\\n    year={2022}\\n}",
            "permalink": "arxiv-beyond-exabrick",
            "date": "2022-11-18"
        },
        {
            "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
            "venue": "@VIS",
            "award": "Best Paper Honorable Mentions",
            "authors": "David Bauer, Qi Wu, and Kwan-Liu Ma",
            "permalink": "vis2022-fovonet",
            "preprint_url": "https://drive.google.com/file/d/1-H7rtC_VH88oNp10kKSCacFE3IU2VCjj/view?usp=sharing",
            "official_url": "https://doi.org/10.1109/TVCG.2022.3209498",
            "preview": "vis2022-fovolnet.png",
            "teaser": "vis2022-fovolnet-teaser.png",
            "teaser_caption": "Figure 1: We propose a novel rendering pipeline for fast volume rendering using optimized foveated sparse rendering and deep neural reconstruction networks. FoVolNet can faithfully reconstruct visual information from sparse inputs. With FoVolNet, developers are able to significantly improve rendering time without sacrificing visual quality.",
            "bibtex": "@article{bauer2022fovolnet,\\n    author = {D. Bauer and Q. Wu and K. Ma},\\n    journal = {IEEE Transactions on Visualization &amp; Computer Graphics},\\n    title = {FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks},\\n    year = {2023},\\n    volume = {29},\\n    number = {01},\\n    issn = {1941-0506},\\n    pages = {515-525},\\n    doi = {10.1109/TVCG.2022.3209498},\\n    publisher = {IEEE Computer Society},\\n    address = {Los Alamitos, CA, USA},\\n    month = {jan}\\n}",
            "date": "2022-09-26"
        },
        {
            "title": "Instant Neural Representation for Interactive Volume Rendering and Volume Compression",
            "venue": "Submitted for Publication",
            "authors": "Qi Wu, David Bauer, Michael J. Doyle, and Kwan-Liu Ma",
            "abstract": "Neural networks have shown great potential in compressing volume data for visualization. However, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. In this paper, we demonstrate that by simultaneously leveraging modern GPU tensor cores, a native CUDA neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). Our neural representations are also high-fidelity (PSNR > 30dB) and compact (10-1000x smaller). Additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. To support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an NVIDIA RTX 3090 workstation.",
            "preprint_url": "https://drive.google.com/file/d/1IyXgzGw7EpHWAsakZoHsE8aU9RqdjHyg/view?usp=sharing",
            "arxiv_url": "https://arxiv.org/abs/2207.11620",
            "preview": "tvcg2023-instant-vnr.png",
            "teaser": "tvcg2023-instant-vnr-teaser.png",
            "teaser_caption": "Figure 1: A) An overview of our work. The sampling step randomly and uniformly generates sample using the ground truth (GT) data. The ground truth data can be loaded via out-of-core streaming. The training step optimizes the neural network. The rendering step renders the neural network via in-shader or sample-streaming methods. Our approach accommodates both pre-training and online-training. Our novel contributions are highlighted in yellow. B) The architecture of our neural network with the multi-resolution hash grid encoding method.",
            "bibtex": "@article{wu2022instant,\\n    title={Instant Neural Representation for Interactive Volume Rendering},\\n    author={Wu, Qi and Doyle, Michael J and Bauer, David and Ma, Kwan-Liu},\\n    journal={arXiv preprint arXiv:2207.11620},\\n    year={2022}\\n}",
            "permalink": "tvcg2023-instant-vnr",
            "date": "2022-10-24"
        },
        {
            "title": "A Flexible Data Streaming Design for Interactive Visualization of Large-Scale Volume Data",
            "venue": "@EGPGV",
            "authors": "Qi Wu, Michael J. Doyle, and Kwan-Liu Ma",
            "preprint_url": "https://drive.google.com/file/d/13laOefDThpDUr-nxfpjTVLdTI2tiNpCi/view?usp=sharing",
            "official_url": "https://doi.org/10.2312/pgv.20221064",
            "preview": "egpgv2022-streaming-volume.png",
            "teaser": "egpgv2022-streaming-volume-teaser.png",
            "teaser_caption": "Figure 1: Large-scale progressive volume rendering of the deep ocean water asteroid impact dataset. A) In our system, the progressive rendering is done by breaking the volume interval into smaller segments, and only compute one segment per frame. B) Additionally, our system can also break the framebuffer into smaller tiles, and only render one tile at a time. Both method allows our rendering system to significantly reduce memory footprints.",
            "bibtex": "@inproceedings{wu2022flexible,\\n    booktitle = {Eurographics Symposium on Parallel Graphics and Visualization},\\n    editor = {Bujack, Roxana and Tierny, Julien and Sadlo, Filip},\\n    title = {{A Flexible Data Streaming Design for Interactive Visualization of Large-Scale Volume Data}},\\n    author = {Wu, Qi and Doyle, Michael J. and Ma, Kwan-Liu},\\n    year = {2022},\\n    publisher = {The Eurographics Association},\\n    ISSN = {1727-348X},\\n    ISBN = {978-3-03868-175-5},\\n    DOI = {10.2312/pgv.20221064}\\n}",
            "permalink": "egpgv2022-streaming-volume",
            "date": "2022-06-13"
        },
        {
            "title": "Distributed Neural Representation for Reactive in situ Visualization",
            "venue": "@LDAV_poster",
            "authors": "Qi Wu, Joseph A. Insley, Victor A. Mateevitsi, Silvio Rizzi, and Kwan-Liu Ma",
            "abstract": "Volumetric grids have recently been used by many recent works for representing complex scenes implicitly. A volumetric neural representation can be several orders of magnitude smaller in size while still preserving most of high-frequency details. However, most volumes used in large-scale in situ visualization and analysis are partitioned and generated directly in parallel. Therefore, a compatible technique to create volumetric neural representations for these situations is much needed. In this project, we explore the possibility of constructing and optimizing such a representation for large-scale distributed volumes. We present our preliminary results in this poster. We also outline our plans to integrate our techniques with existing in situ visualization and analysis pipelines.",
            "preprint_url": "https://drive.google.com/file/d/1Qhtzp42clC797zTnx9Lj4QkfYQJYX8kU/view?usp=share_link",
            "official_url": "https://doi.org/10.1109/LDAV57265.2022.9966405",
            "preview": "ldav2022-dvnr-poster.png",
            "teaser": "ldav2022-dvnr-poster-teaser.png",
            "teaser_caption": "Figure 1: The training and evaluation setup described in this poster. As a proof of concept, we use a regular volume that is pre-partitioned into two. Ghost voxels are also computed during the partitioning process.",
            "bibtex": "@inproceedings{9966405,\\n    author={Wu, Qi and Insley, Joseph A. and Mateevitsi, Victor A. and Rizzi, Silvio and Ma, Kwan-Liu},\\n    booktitle={2022 IEEE 12th Symposium on Large Data Analysis and Visualization (LDAV)}, \\n    title={Distributed Volumetric Neural Representation for in situ Visualization and Analysis}, \\n    year={2022},\\n    volume={},\\n    number={},\\n    pages={1-2},\\n    doi={10.1109/LDAV57265.2022.9966405}\\n}",
            "permalink": "ldav2022-dvnr-poster",
            "date": "2022-06-13"
        },
        {
            "title": "DIVA: A Declarative and Reactive Language for in situ Visualization",
            "venue": "@LDAV",
            "authors": "Qi Wu, Tyson Neuroth, Oleg Igouchkine, Konduri Aditya, Jacqueline H. Chen, and Kwan-Liu Ma",
            "abstract": "The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.",
            "preprint_url": "https://drive.google.com/file/d/1i_D4LDDRNM_CSOoTgT6iE3a6D2yzbUr4/view?usp=sharing",
            "official_url": "https://doi.org/10.1109/LDAV51489.2020.00007",
            "preview": "ldav2020-diva.png",
            "teaser": "ldav2020-diva-teaser.png",
            "teaser_caption": "Figure 1: A DIVA program is processed through three layers. Users typically specify their program using the declarative interface (left); then the language parser will translate it into an internal DAG representation; this representation will then be interpreted into a low-level dataflow API for execution. A) A DIVA program computes a volume rendering for every 5 timesteps, and saves the rendering on disk. B) The same program in the DAG representation. C) The same program in the low-level API. Because the C++ API is not declarative, in part C), statements have to be executed in order. Moreover, because C++ does not track data dependencies automatically, all variables declared in C) should be wrapped by lifting operators (e.g., divaCreateSource). D) The hierarchy of primitives defined by the low-level dataflow API: All values in DIVA are signals; values depending on external inputs are sources; values returning to the environment are actions (e.g., a saved image file); triggers are special primitives that decide which actions to compute based on predicates; rest values are internal to the workflow and are represented by either pure (i.e., DivaPureOp) or impure functions (i.e., DivaImpure).",
            "bibtex": "@inproceedings{wu2020diva,\\n    title={{DIVA}: A Declarative and Reactive Language for in situ Visualization},\\n    author={Wu, Qi and Neuroth, Tyson and Igouchkine, Oleg and Aditya, Konduri and Chen, Jacqueline H and Ma, Kwan-Liu},\\n    booktitle={2020 IEEE 10th Symposium on Large Data Analysis and Visualization (LDAV)},\\n    pages={1--11},\\n    year={2020},\\n    organization={IEEE}\\n}",
            "permalink": "ldav2020-diva",
            "date": "2020-10-25"
        },
        {
            "title": "VisIt-OSPRay: toward an exascale volume visualization system",
            "venue": "@EGPGV",
            "authors": "Qi Wu, Will Usher, Steve Petruzza, Sidharth Kumar, Feng Wang, Ingo Wald, Valerio Pascucci, and Charles D. Hansen",
            "preprint_url": "https://drive.google.com/file/d/1ulrP4ogjIoxCR2A_oE9afemqhyK04lLA/view?usp=sharing",
            "official_url": "https://dl.acm.org/doi/10.5555/3293524.3293526",
            "preview": "egpgv2018-visit-ospray.png",
            "teaser": "egpgv2018-visit-ospray-teaser.png",
            "teaser_caption": "Figure 1: High-quality interactive volume visualization using VisIt-OSPRay: a) volume rendering of O2 concentration inside a combustion chamber; b) volume rendering of the Richtmyer-Meshkov Instability; c) visualization of a supernova simulation; d) visualization of the aneurysm dataset using volume rendering and streamlines; e) scalable volume rendering of the 966GB DNS data on 64 Stampede2 Intel® Xeon Phi™ Knight’s Landing nodes.",
            "bibtex": "@inproceedings{wu2018visit,\\n    author = {Wu, Qi and Usher, Will and Petruzza, Steve and Kumar, Sidharth and Wang, Feng and Wald, Ingo and Pascucci, Valerio and Hansen, Charles D.},\\n    title = {{VisIt-OSPRay}: Toward an Exascale Volume Visualization System},\\n    year = {2018},\\n    publisher = {Eurographics Association},\\n    address = {Goslar, DEU},\\n    booktitle = {Proceedings of the Symposium on Parallel Graphics and Visualization},\\n    pages = {13–24},\\n    numpages = {12},\\n    location = {Brno, Czech Republic},\\n    series = {EGPGV '18}\\n}",
            "permalink": "egpgv2018-visit-ospray",
            "date": "2018-06-04"
        },
        {
            "title": "CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data",
            "venue": "@VIS",
            "authors": "Feng Wang, Ingo Wald, Qi Wu, Will Usher, and Chris R. Johnson",
            "preprint_url": "https://drive.google.com/file/d/1cNUeMO8X8hUjtTWlrVxslmeLOdq3vdyJ/view?usp=sharing",
            "official_url": "https://doi.org/10.1109/TVCG.2018.2864850",
            "preview": "vis2018-impl.png",
            "teaser": "vis2018-impl-teaser.png",
            "teaser_caption": "Figure 1: We propose a novel rendering pipeline for fast volume rendering using optimized foveated sparse rendering and deep neural reconstruction networks. FoVolNet can faithfully reconstruct visual information from sparse inputs. With FoVolNet, developers are able to significantly improve rendering time without sacrificing visual quality.",
            "bibtex": "@article{wang2018cpu,\\n    author={Wang, Feng and Wald, Ingo and Wu, Qi and Usher, Will and Johnson, Chris R.},\\n    journal={IEEE Transactions on Visualization and Computer Graphics}, \\n    title={CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data}, \\n    year={2019},\\n    volume={25},\\n    number={1},\\n    pages={1142-1151},\\n    doi={10.1109/TVCG.2018.2864850}\\n}",
            "permalink": "vis2018-impl",
            "date": "2018-10-16"
        },
        {
            "title": "Ray Tracing Generalized Tube Primitives: Method and Applications",
            "venue": "@EuroVis",
            "authors": "Mengjiao Han, Ingo Wald, Will Usher, Qi Wu, Feng Wang, Valerio Pascucci, Charles D. Hansen, and Chris R. Johnson",
            "preprint_url": "https://drive.google.com/file/d/17pB_tIKD3jF6bVT1u9wpx9dwO_MqD-n6/view?usp=sharing",
            "official_url": "https://doi.org/10.1111/cgf.13703",
            "preview": "eurovis2019-tubes.png",
            "teaser": "eurovis2019-tubes-teaser.png",
            "teaser_caption": "Figure 1: Visualizations using our 'generalized tube' primitives. (a): DTI tractography data, semi-transparent fixed-radius streamlines (218K line segments). (b): A generated neuron assembly test case, streamlines with varying radii and bifurcations (3.2M l. s.). (c): Aneurysm morphology, semi-transparent streamlines with varying radii and bifurcations (3.9K l. s.) and an opaque center line with fixed radius and bifurcations (3.9K l. s.). (d): A tornado simulation, withradius used to encode the velocity magnitude (3.56M l. s.). (e): Flow past a torus, fixed-radiuspathlines (6.5M l. s.). Rendered at: (a) 0.38FPS, (b) 7.2FPS, (c) 0.25FPS, (d) 18.8FPS, with a 20482 framebuffer; (e) 23FPS with a 2048 x 786 framebuffer Performance measured on a dual Intel® Xeon® E5-2640 v4 workstation, with shadows and ambient occlusion.",
            "bibtex": "@inproceedings{han2019ray,\\n    title={Ray tracing generalized tube primitives: Method and applications},\\n    author={Han, Mengjiao and Wald, Ingo and Usher, Will and Wu, Qi and Wang, Feng and Pascucci, Valerio and Hansen, Charles D. and Johnson, Chris R.},\\n    booktitle={Computer Graphics Forum},\\n    volume={38},\\n    number={3},\\n    pages={467--478},\\n    year={2019},\\n    organization={Wiley Online Library}\\n}",
            "permalink": "eurovis2019-tubes",
            "date": "2019-06-01"
        },
        {
            "title": "Thermodynamic versus kinetic control in self-assembly of zero-, one-, quasitwo-, and two-dimensional metal-organic coordination structures",
            "venue": "The Journal of Chemical Physics",
            "authors": "Tao Lin, Qi Wu, Jun Liu, Ziliang Shi, Pei Nian Liu, and Nian Lin",
            "abstract": "Four types of metal-organic structures exhibiting specific dimensionality were studied using scanning tunneling microscopy and Monte Carlo simulations. The four structures were self-assembled out of specifically designed molecular building blocks via the same coordination motif on an Au(111) surface. We found that the four structures behaved differently in response to thermal annealing treatments: The two-dimensional structure was under thermodynamic control while the structures of lower dimension were under kinetic control. Monte Carlo simulations revealed that the self-assembly pathways of the four structures are associated with the characteristic features of their specific heat. These findings provide insights into how the dimensionality of supramolecular coordination structures affects their thermodynamic properties.",
            "preprint_url": "https://drive.google.com/file/d/1IHYu8y6lfB9eqC8M7Ji_iCbmhGZ8xJ4V/view?usp=share_link",
            "official_url": "https://doi.org/10.1063/1.4906174",
            "preview": "jcp-stm.png",
            "teaser": "jcp-stm-teaser.png",
            "teaser_caption": "Figure 1. Left: Representative STM images of the four bi-component systems following a series of thermal annealing treatments. (a)-(c) 1-5. (d)-(f) 2-5. (g)-(i) 3-5. (j)-(l) 4-5. The annealing temperature is indicated in each image. All images are 100 x 100 nm2.  Right: MC simulated structures of the four bi-component systems formed at different temperatures. (a)-(d) 1-5. (e)-(h) 2-5. (i)-(l) 3-5. (m)-(p) 4-5.",
            "permalink": "jcp-stm",
            "date": "2015-01-01"
        }
    ]
}
